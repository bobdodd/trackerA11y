# TrackerA11y - Comprehensive Accessibility Testing Platform

## Project Overview

TrackerA11y is a unified accessibility testing platform that combines real-time interaction tracking, screen structure analysis, and integrated diarized audio processing to provide the most comprehensive view of user experience during accessibility testing. By implementing a hybrid TypeScript-Python architecture, TrackerA11y becomes the definitive tool for understanding both what users do and what they think during accessibility testing sessions.

## Project Vision

TrackerA11y represents a paradigm shift from fragmented accessibility testing tools to a unified platform that provides unprecedented insight into user experience. Unlike existing solutions that require complex tool integration and post-hoc correlation, TrackerA11y captures and correlates all aspects of user interaction in real-time with microsecond precision.

## Hybrid Architecture Benefits

### TypeScript Core + Python Audio Pipeline

**Why This Approach?**
- **TypeScript Core**: Optimal for real-time system integration, cross-platform APIs, and web technology integration
- **Python Audio Pipeline**: Leverages ML/AI ecosystem for state-of-the-art audio processing
- **Best of Both Worlds**: Each language used for its optimal strengths

**Key Benefits:**
- **Elimination of synchronization complexity**: Native integration prevents data loss and timing errors
- **Real-time correlation**: Immediate insights as testing progresses  
- **Higher accuracy**: Purpose-built components optimized for their specific tasks
- **Simplified workflow**: Single installation, unified reporting, seamless operation
- **Enhanced analysis**: Capabilities impossible with fragmented tools
- **Scalable performance**: Independent scaling of audio processing and system monitoring

### Comprehensive Capabilities

#### Enhanced Interaction Tracking (TypeScript Core)
1. **Application Focus Tracking**: Monitor which application has focus across all platforms
2. **Advanced Mouse Event Capture**: Complete mouse button support (left, right, middle, additional) with modifier keys
   - **Modifier Key Support**: Capture Command+click, Shift+click, Control+click, Option+click combinations
   - **Comprehensive Button Support**: All mouse buttons including middle-click and additional buttons
   - **Enhanced Event Types**: Mouse down/up, drag, hover, dwell time tracking
3. **Dock Integration**: Native macOS dock detection and interaction tracking
   - **Universal Dock Detection**: Works with dock in any position (bottom, left, right)
   - **Application Icon Tracking**: Identify specific applications being launched from dock
   - **Coordinate-Based Detection**: Robust detection without hanging or timeouts
4. **Browser Element Detection**: Deep DOM inspection for web accessibility testing
   - **Cross-Browser Support**: Safari, Chrome, Firefox integration
   - **Real-Time Element Identification**: Instant detection of clicked web elements
   - **Coordinate Mapping**: Accurate screen-to-browser coordinate transformation
5. **Hover & Dwell Time Analysis**: Advanced interaction timing analysis
   - **Hover Detection**: 500ms threshold with 10px movement tolerance
   - **Dwell Time Measurement**: Precise timing of user hesitation and exploration
   - **Movement Tracking**: Sub-pixel precision mouse movement analysis
6. **Page/Screen Structure Analysis**: Deep inspection of accessibility trees and DOM structures
7. **Screen Recording Integration**: Synchronized visual capture with interaction overlays

#### Integrated Audio Analysis (Python Pipeline)
8. **High-Quality Audio Recording**: Professional-grade 48kHz/96kHz lossless audio capture
9. **Speaker Diarization**: Advanced AI-powered separation using pyannote/speaker-diarization-3.1
10. **Role-Aware Transcription**: Specialized transcription with Whisper Large v3 optimized for each speaker
11. **Think-Aloud Analysis**: Deep analysis of user cognitive processes and mental models
12. **Sentiment & Emotion Tracking**: Real-time detection of user frustration, confusion, and satisfaction

#### Intelligence & Correlation (Hybrid System)
13. **Real-Time Correlation**: Immediate linking of user speech with their actions via IPC bridge
14. **Accessibility Issue Detection**: Automated identification of barriers through audio-visual correlation
15. **Intent Analysis**: Understanding user goals and expectations from speech-action patterns
16. **Comprehensive Reporting**: Multi-format reports with unified audio-visual-interaction insights

### Technical Excellence

#### Performance & Scale
- **Cross-platform support**: Desktop (Windows, macOS, Linux) and mobile (iOS, Android)
- **Microsecond precision**: High-performance timestamping and event correlation
- **Non-intrusive operation**: Minimal impact on testing experience with optimized resource usage
- **Professional audio quality**: Configurable 48kHz/96kHz recording with advanced noise reduction

#### AI & Machine Learning
- **State-of-the-art diarization**: pyannote/speaker-diarization-3.1 for accurate speaker separation
- **Advanced transcription**: Whisper Large v3 with role-specific optimization and confidence scoring
- **Intelligent correlation**: ML-powered linking of speech patterns to interaction events
- **Accessibility expertise**: Domain-specific models trained on accessibility testing scenarios

## Project Structure

```
trackerA11y/
‚îú‚îÄ‚îÄ src/                          # TypeScript Core
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # Core system orchestration
‚îÇ   ‚îú‚îÄ‚îÄ platforms/                # Platform-specific implementations
‚îÇ   ‚îú‚îÄ‚îÄ bridge/                   # Python IPC communication
‚îÇ   ‚îú‚îÄ‚îÄ types/                    # TypeScript type definitions
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Utility functions
‚îú‚îÄ‚îÄ audio_pipeline/               # Python Audio Processing
‚îÇ   ‚îú‚îÄ‚îÄ src/audio_pipeline/       # Python source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processors/           # Audio analysis engines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/               # Data models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ communication/        # IPC handling
‚îÇ   ‚îî‚îÄ‚îÄ tests/                    # Python tests
‚îú‚îÄ‚îÄ docs/                         # Comprehensive documentation
‚îî‚îÄ‚îÄ tests/                        # TypeScript tests
```

## Getting Started

### Prerequisites

**TypeScript Core:**
- Node.js 18+
- npm or yarn
- Platform-specific development tools (Xcode, Visual Studio, etc.)

**Python Audio Pipeline:**
- Python 3.9+
- pip or poetry
- PyTorch (for ML models)
- Audio system access

### Installation

1. **Install TypeScript dependencies:**
```bash
npm install
```

2. **Install Python audio pipeline:**
```bash
cd audio_pipeline
pip install -r requirements.txt
# or with poetry
poetry install
```

3. **Build TypeScript core:**
```bash
npm run build
```

4. **Run tests:**
```bash
# TypeScript tests
npm test

# Python tests  
cd audio_pipeline
pytest
```

### Development

**Start development mode:**
```bash
# Terminal 1: TypeScript development server
npm run dev

# Terminal 2: Python pipeline in development mode
cd audio_pipeline
python -m audio_pipeline.main --mode standalone --log-level DEBUG
```

## Enhanced Usage Examples

### Quick Start: Enhanced Demo
Experience all the new enhanced features with the comprehensive demo:

```bash
# Run the enhanced accessibility demo with all features
npm run demo:enhanced
```

This demo showcases:
- üñ±Ô∏è **Advanced Mouse Tracking**: All button support with modifier keys
- üö¢ **Dock Detection**: Native macOS dock interaction tracking  
- üåê **Browser Integration**: Real-time DOM element detection
- ü´ß **Hover Analysis**: Dwell time and movement precision
- ‚å®Ô∏è **Enhanced Output**: Clean, emoji-coded interaction logging

### Recording with Enhanced Features
Start a comprehensive recording session:

```bash
# Record everything with enhanced tracking
npm run record
```

### Accessibility-Only Demo
For focused accessibility testing:

```bash
# Original accessibility demo (clean output)
npm run demo:accessibility
```

### Enhanced Feature Examples

**Dock Interactions:**
```
üñ±Ô∏è CLICK [Command] #1 | üö¢ Dock: Terminal
‚¨áÔ∏è DOCK PRESS | üö¢ Dock: Safari
üñ±Ô∏è RIGHT CLICK #2 | üö¢ Dock: Chrome
```

**Browser Element Detection:**
```
üñ±Ô∏è CLICK #3 | <button> "Submit Form"
üñ±Ô∏è CLICK [Shift] #4 | <a> "Learn More" (https://example.com...)
‚¨áÔ∏è PRESS LEFT [Control] | <input>#search "Search for..."
```

**Enhanced Interactions:**
```
ü´ß HOVER START | <div> "Navigation Menu"
ü´ß HOVER END (1250ms)
ü´≥ DRAG LEFT [Option] | <div> "Draggable Item"
‚¨ÜÔ∏è SCROLL [Command] | <main> "Main Content"
```

## Architecture Deep Dive

For detailed information about our architecture decisions and implementation:

- **[Architecture Decisions](./docs/architecture-decisions.md)** - Comprehensive ADR for hybrid TypeScript-Python approach
- **[Application Focus Tracking](./docs/app-focus-tracking.md)** - Platform-specific APIs and implementation patterns
- **[User Interaction Tracking](./docs/user-interaction-tracking.md)** - Cross-platform interaction monitoring
- **[Screen Structure Analysis](./docs/screen-structure-analysis.md)** - Accessibility tree analysis and DOM inspection
- **[Audio Synchronization](./docs/audio-synchronization.md)** - High-precision timestamping and correlation
- **[Implementation Guide](./docs/implementation-guide.md)** - Production-ready architecture and code examples

## Development Roadmap

### Phase 1: Foundation (Months 1-3) üöß *In Progress*
- [x] **Project Structure**: Hybrid TypeScript-Python architecture setup
- [x] **Documentation**: Architecture decisions and technical foundations  
- [ ] **Cross-Platform Tracking Core**: Application focus and interaction monitoring
- [ ] **Audio Infrastructure**: Recording and diarization pipeline with IPC bridge
- [ ] **Correlation Engine**: Real-time event linking and timestamping system
- [ ] **Data Models**: Unified data structures for multi-modal analysis

### Phase 2: Intelligence (Months 4-6)
- [ ] **Advanced Transcription**: Role-aware speech processing with Whisper Large v3
- [ ] **Accessibility Analysis**: AI-powered barrier detection from audio-visual correlation
- [ ] **Screen Structure Analysis**: Complete accessibility tree monitoring and change detection
- [ ] **Performance Optimization**: Efficient data processing and storage systems

### Phase 3: Platform (Months 7-12)
- [ ] **API Development**: Comprehensive developer APIs and SDKs
- [ ] **Integration Ecosystem**: Connectors for CI/CD, testing frameworks, and research tools
- [ ] **Advanced Analytics**: ML-powered insights and automated reporting
- [ ] **Enterprise Features**: Real-time monitoring, compliance reporting, and team collaboration

## Contributing

We welcome contributions to TrackerA11y! Please see our contributing guidelines and:

1. Check existing issues and pull requests
2. Follow our code style (ESLint for TypeScript, Black for Python)
3. Add tests for new functionality
4. Update documentation as needed

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- **pyannote.audio team** for state-of-the-art speaker diarization
- **OpenAI Whisper team** for advanced speech recognition
- **Accessibility community** for inspiration and domain expertise

---

**TrackerA11y**: Where accessibility testing meets artificial intelligence. ü§ñ‚ôø